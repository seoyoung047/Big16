{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **20230613**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 연습(맛보기)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential  # 텐서플로의 케라스 API에서 필요한 함수들을 불러옵니다.\n",
    "from tensorflow.keras.layers import Dense       # 데이터를 다루는데 필요한 라이브러리를 불러옵니다.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로우 안에는 무조건 ndarray 형태의 데이터 넣어야함\n",
    "#       => np.loadtxt()\n",
    "Data_set = np.loadtxt(\"../data/ThoraricSurgery3.csv\", delimiter=\",\") # 준비된 수술 환자 데이터를 불러옵니다.\n",
    "X = Data_set[:,0:16]    # 환자의 진찰 기록을 X로 지정합니다.\n",
    "y = Data_set[:,16]      # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(470, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(470,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 구축"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 신경망 구조 설정\n",
    "\n",
    "- Squential() 생성\n",
    "\n",
    "\n",
    "- 층(Dense) 구성\n",
    "    - 입력층\n",
    "    - 히든층\n",
    "    - 출력층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 구성하기\n",
    "\n",
    "# 히든층이 없는 구조\n",
    "model = Sequential()         # 딥러닝 모델의 구조를 결정합니다.\n",
    "\n",
    "# 입력층\n",
    "#                   units : 신경 기억회로 개수 지정\n",
    "#                   input_dim:입력되는 데이터 개수 지정(=입력층 차원개수) \n",
    "#                   activation: (relu=비선형)\n",
    "model.add(Dense(30, input_dim=16, activation='relu')) \n",
    "\n",
    "\n",
    "# 출력층\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model.add(Dense(,,,)) : 신경망 층 만들기\n",
    "\n",
    "```python\n",
    "ex)\n",
    "model.add(Dense(30, input_dim=16, activation='relu')) \n",
    "```\n",
    "\n",
    "- h $\\Leftarrow$ units=30\n",
    "- x $\\Leftarrow$ imput_dim=16\n",
    "\n",
    "$ h_1 = w_1 * x_1 + w_2 * x_2 + \\cdots + w_{16} * x_{16} + w_b$ <br>\n",
    "$\\cdots h_{30}$\n",
    "\n",
    "\n",
    "$$ h_i = \\sum_{i=1}^{30}{W_i * X^T_i} + w_{bi} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0muse_bias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'glorot_uniform'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbias_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'zeros'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mkernel_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbias_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mactivity_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mkernel_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbias_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Just your regular densely-connected NN layer.\n",
      "\n",
      "`Dense` implements the operation:\n",
      "`output = activation(dot(input, kernel) + bias)`\n",
      "where `activation` is the element-wise activation function\n",
      "passed as the `activation` argument, `kernel` is a weights matrix\n",
      "created by the layer, and `bias` is a bias vector created by the layer\n",
      "(only applicable if `use_bias` is `True`). These are all attributes of\n",
      "`Dense`.\n",
      "\n",
      "Note: If the input to the layer has a rank greater than 2, then `Dense`\n",
      "computes the dot product between the `inputs` and the `kernel` along the\n",
      "last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).\n",
      "For example, if input has dimensions `(batch_size, d0, d1)`,\n",
      "then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates\n",
      "along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)`\n",
      "(there are `batch_size * d0` such sub-tensors).\n",
      "The output in this case will have shape `(batch_size, d0, units)`.\n",
      "\n",
      "Besides, layer attributes cannot be modified after the layer has been called\n",
      "once (except the `trainable` attribute).\n",
      "When a popular kwarg `input_shape` is passed, then keras will create\n",
      "an input layer to insert before the current layer. This can be treated\n",
      "equivalent to explicitly defining an `InputLayer`.\n",
      "\n",
      "Example:\n",
      "\n",
      ">>> # Create a `Sequential` model and add a Dense layer as the first layer.\n",
      ">>> model = tf.keras.models.Sequential()\n",
      ">>> model.add(tf.keras.Input(shape=(16,)))\n",
      ">>> model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
      ">>> # Now the model will take as input arrays of shape (None, 16)\n",
      ">>> # and output arrays of shape (None, 32).\n",
      ">>> # Note that after the first layer, you don't need to specify\n",
      ">>> # the size of the input anymore:\n",
      ">>> model.add(tf.keras.layers.Dense(32))\n",
      ">>> model.output_shape\n",
      "(None, 32)\n",
      "\n",
      "Args:\n",
      "  units: Positive integer, dimensionality of the output space.\n",
      "  activation: Activation function to use.\n",
      "    If you don't specify anything, no activation is applied\n",
      "    (ie. \"linear\" activation: `a(x) = x`).\n",
      "  use_bias: Boolean, whether the layer uses a bias vector.\n",
      "  kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      "  bias_initializer: Initializer for the bias vector.\n",
      "  kernel_regularizer: Regularizer function applied to\n",
      "    the `kernel` weights matrix.\n",
      "  bias_regularizer: Regularizer function applied to the bias vector.\n",
      "  activity_regularizer: Regularizer function applied to\n",
      "    the output of the layer (its \"activation\").\n",
      "  kernel_constraint: Constraint function applied to\n",
      "    the `kernel` weights matrix.\n",
      "  bias_constraint: Constraint function applied to the bias vector.\n",
      "\n",
      "Input shape:\n",
      "  N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      "  The most common situation would be\n",
      "  a 2D input with shape `(batch_size, input_dim)`.\n",
      "\n",
      "Output shape:\n",
      "  N-D tensor with shape: `(batch_size, ..., units)`.\n",
      "  For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      "  the output would have shape `(batch_size, units)`.\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\user\\anaconda3\\envs\\dl-dev-cpu\\lib\\site-packages\\keras\\layers\\core\\dense.py\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     Dense"
     ]
    }
   ],
   "source": [
    "Dense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 30)                510       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 541\n",
      "Trainable params: 541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 신경망 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "              loss='binary_crossentropy' # 오차 구하는 법 = '이진분류'\n",
    "              , optimizer='adam'         # 경사하강법 방법 = '' \n",
    "              , metrics=['accuracy']     # 평가 방법 = ['정확도']\n",
    "              )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30/30 [==============================] - 1s 2ms/step - loss: 1.1818 - accuracy: 0.8255\n",
      "Epoch 2/5\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.5368 - accuracy: 0.8255\n",
      "Epoch 3/5\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.4617 - accuracy: 0.8383\n",
      "Epoch 4/5\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.4374 - accuracy: 0.8511\n",
      "Epoch 5/5\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.4325 - accuracy: 0.8511\n"
     ]
    }
   ],
   "source": [
    "# = w 구하기\n",
    "#                         epochs : clossvalidation\n",
    "history = model.fit(X, y, epochs=5, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mSequential\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvalidation_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      "\n",
      "Args:\n",
      "    x: Input data. It could be:\n",
      "      - A Numpy array (or array-like), or a list of arrays\n",
      "        (in case the model has multiple inputs).\n",
      "      - A TensorFlow tensor, or a list of tensors\n",
      "        (in case the model has multiple inputs).\n",
      "      - A dict mapping input names to the corresponding array/tensors,\n",
      "        if the model has named inputs.\n",
      "      - A `tf.data` dataset. Should return a tuple\n",
      "        of either `(inputs, targets)` or\n",
      "        `(inputs, targets, sample_weights)`.\n",
      "      - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
      "        or `(inputs, targets, sample_weights)`.\n",
      "      - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a\n",
      "        callable that takes a single argument of type\n",
      "        `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.\n",
      "        `DatasetCreator` should be used when users prefer to specify the\n",
      "        per-replica batching and sharding logic for the `Dataset`.\n",
      "        See `tf.keras.utils.experimental.DatasetCreator` doc for more\n",
      "        information.\n",
      "      A more detailed description of unpacking behavior for iterator types\n",
      "      (Dataset, generator, Sequence) is given below. If using\n",
      "      `tf.distribute.experimental.ParameterServerStrategy`, only\n",
      "      `DatasetCreator` type is supported for `x`.\n",
      "    y: Target data. Like the input data `x`,\n",
      "      it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      "      It should be consistent with `x` (you cannot have Numpy inputs and\n",
      "      tensor targets, or inversely). If `x` is a dataset, generator,\n",
      "      or `keras.utils.Sequence` instance, `y` should\n",
      "      not be specified (since targets will be obtained from `x`).\n",
      "    batch_size: Integer or `None`.\n",
      "        Number of samples per gradient update.\n",
      "        If unspecified, `batch_size` will default to 32.\n",
      "        Do not specify the `batch_size` if your data is in the\n",
      "        form of datasets, generators, or `keras.utils.Sequence` instances\n",
      "        (since they generate batches).\n",
      "    epochs: Integer. Number of epochs to train the model.\n",
      "        An epoch is an iteration over the entire `x` and `y`\n",
      "        data provided\n",
      "        (unless the `steps_per_epoch` flag is set to\n",
      "        something other than None).\n",
      "        Note that in conjunction with `initial_epoch`,\n",
      "        `epochs` is to be understood as \"final epoch\".\n",
      "        The model is not trained for a number of iterations\n",
      "        given by `epochs`, but merely until the epoch\n",
      "        of index `epochs` is reached.\n",
      "    verbose: 'auto', 0, 1, or 2. Verbosity mode.\n",
      "        0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "        'auto' defaults to 1 for most cases, but 2 when used with\n",
      "        `ParameterServerStrategy`. Note that the progress bar is not\n",
      "        particularly useful when logged to a file, so verbose=2 is\n",
      "        recommended when not running interactively (eg, in a production\n",
      "        environment).\n",
      "    callbacks: List of `keras.callbacks.Callback` instances.\n",
      "        List of callbacks to apply during training.\n",
      "        See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n",
      "        and `tf.keras.callbacks.History` callbacks are created automatically\n",
      "        and need not be passed into `model.fit`.\n",
      "        `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
      "        `verbose` argument to `model.fit`.\n",
      "        Callbacks with batch-level calls are currently unsupported with\n",
      "        `tf.distribute.experimental.ParameterServerStrategy`, and users are\n",
      "        advised to implement epoch-level calls instead with an appropriate\n",
      "        `steps_per_epoch` value.\n",
      "    validation_split: Float between 0 and 1.\n",
      "        Fraction of the training data to be used as validation data.\n",
      "        The model will set apart this fraction of the training data,\n",
      "        will not train on it, and will evaluate\n",
      "        the loss and any model metrics\n",
      "        on this data at the end of each epoch.\n",
      "        The validation data is selected from the last samples\n",
      "        in the `x` and `y` data provided, before shuffling. This argument is\n",
      "        not supported when `x` is a dataset, generator or\n",
      "        `keras.utils.Sequence` instance.\n",
      "        If both `validation_data` and `validation_split` are provided,\n",
      "        `validation_data` will override `validation_split`.\n",
      "        `validation_split` is not yet supported with\n",
      "        `tf.distribute.experimental.ParameterServerStrategy`.\n",
      "    validation_data: Data on which to evaluate\n",
      "        the loss and any model metrics at the end of each epoch.\n",
      "        The model will not be trained on this data. Thus, note the fact\n",
      "        that the validation loss of data provided using `validation_split`\n",
      "        or `validation_data` is not affected by regularization layers like\n",
      "        noise and dropout.\n",
      "        `validation_data` will override `validation_split`.\n",
      "        `validation_data` could be:\n",
      "          - A tuple `(x_val, y_val)` of Numpy arrays or tensors.\n",
      "          - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays.\n",
      "          - A `tf.data.Dataset`.\n",
      "          - A Python generator or `keras.utils.Sequence` returning\n",
      "          `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n",
      "        `validation_data` is not yet supported with\n",
      "        `tf.distribute.experimental.ParameterServerStrategy`.\n",
      "    shuffle: Boolean (whether to shuffle the training data\n",
      "        before each epoch) or str (for 'batch'). This argument is ignored\n",
      "        when `x` is a generator or an object of tf.data.Dataset.\n",
      "        'batch' is a special option for dealing\n",
      "        with the limitations of HDF5 data; it shuffles in batch-sized\n",
      "        chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
      "    class_weight: Optional dictionary mapping class indices (integers)\n",
      "        to a weight (float) value, used for weighting the loss function\n",
      "        (during training only).\n",
      "        This can be useful to tell the model to\n",
      "        \"pay more attention\" to samples from\n",
      "        an under-represented class.\n",
      "    sample_weight: Optional Numpy array of weights for\n",
      "        the training samples, used for weighting the loss function\n",
      "        (during training only). You can either pass a flat (1D)\n",
      "        Numpy array with the same length as the input samples\n",
      "        (1:1 mapping between weights and samples),\n",
      "        or in the case of temporal data,\n",
      "        you can pass a 2D array with shape\n",
      "        `(samples, sequence_length)`,\n",
      "        to apply a different weight to every timestep of every sample. This\n",
      "        argument is not supported when `x` is a dataset, generator, or\n",
      "       `keras.utils.Sequence` instance, instead provide the sample_weights\n",
      "        as the third element of `x`.\n",
      "    initial_epoch: Integer.\n",
      "        Epoch at which to start training\n",
      "        (useful for resuming a previous training run).\n",
      "    steps_per_epoch: Integer or `None`.\n",
      "        Total number of steps (batches of samples)\n",
      "        before declaring one epoch finished and starting the\n",
      "        next epoch. When training with input tensors such as\n",
      "        TensorFlow data tensors, the default `None` is equal to\n",
      "        the number of samples in your dataset divided by\n",
      "        the batch size, or 1 if that cannot be determined. If x is a\n",
      "        `tf.data` dataset, and 'steps_per_epoch'\n",
      "        is None, the epoch will run until the input dataset is exhausted.\n",
      "        When passing an infinitely repeating dataset, you must specify the\n",
      "        `steps_per_epoch` argument. If `steps_per_epoch=-1` the training\n",
      "        will run indefinitely with an infinitely repeating dataset.\n",
      "        This argument is not supported with array inputs.\n",
      "        When using `tf.distribute.experimental.ParameterServerStrategy`:\n",
      "          * `steps_per_epoch=None` is not supported.\n",
      "    validation_steps: Only relevant if `validation_data` is provided and\n",
      "        is a `tf.data` dataset. Total number of steps (batches of\n",
      "        samples) to draw before stopping when performing validation\n",
      "        at the end of every epoch. If 'validation_steps' is None, validation\n",
      "        will run until the `validation_data` dataset is exhausted. In the\n",
      "        case of an infinitely repeated dataset, it will run into an\n",
      "        infinite loop. If 'validation_steps' is specified and only part of\n",
      "        the dataset will be consumed, the evaluation will start from the\n",
      "        beginning of the dataset at each epoch. This ensures that the same\n",
      "        validation samples are used every time.\n",
      "    validation_batch_size: Integer or `None`.\n",
      "        Number of samples per validation batch.\n",
      "        If unspecified, will default to `batch_size`.\n",
      "        Do not specify the `validation_batch_size` if your data is in the\n",
      "        form of datasets, generators, or `keras.utils.Sequence` instances\n",
      "        (since they generate batches).\n",
      "    validation_freq: Only relevant if validation data is provided. Integer\n",
      "        or `collections.abc.Container` instance (e.g. list, tuple, etc.).\n",
      "        If an integer, specifies how many training epochs to run before a\n",
      "        new validation run is performed, e.g. `validation_freq=2` runs\n",
      "        validation every 2 epochs. If a Container, specifies the epochs on\n",
      "        which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
      "        validation at the end of the 1st, 2nd, and 10th epochs.\n",
      "    max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      "        input only. Maximum size for the generator queue.\n",
      "        If unspecified, `max_queue_size` will default to 10.\n",
      "    workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      "        only. Maximum number of processes to spin up\n",
      "        when using process-based threading. If unspecified, `workers`\n",
      "        will default to 1.\n",
      "    use_multiprocessing: Boolean. Used for generator or\n",
      "        `keras.utils.Sequence` input only. If `True`, use process-based\n",
      "        threading. If unspecified, `use_multiprocessing` will default to\n",
      "        `False`. Note that because this implementation relies on\n",
      "        multiprocessing, you should not pass non-picklable arguments to\n",
      "        the generator as they can't be passed easily to children processes.\n",
      "\n",
      "Unpacking behavior for iterator-like inputs:\n",
      "    A common pattern is to pass a tf.data.Dataset, generator, or\n",
      "  tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
      "  yield not only features (x) but optionally targets (y) and sample weights.\n",
      "  Keras requires that the output of such iterator-likes be unambiguous. The\n",
      "  iterator should return a tuple of length 1, 2, or 3, where the optional\n",
      "  second and third elements will be used for y and sample_weight\n",
      "  respectively. Any other type provided will be wrapped in a length one\n",
      "  tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
      "  should still adhere to the top-level tuple structure.\n",
      "  e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
      "  features, targets, and weights from the keys of a single dict.\n",
      "    A notable unsupported data type is the namedtuple. The reason is that\n",
      "  it behaves like both an ordered datatype (tuple) and a mapping\n",
      "  datatype (dict). So given a namedtuple of the form:\n",
      "      `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
      "  it is ambiguous whether to reverse the order of the elements when\n",
      "  interpreting the value. Even worse is a tuple of the form:\n",
      "      `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
      "  where it is unclear if the tuple was intended to be unpacked into x, y,\n",
      "  and sample_weight or passed through as a single element to `x`. As a\n",
      "  result the data processing code will simply raise a ValueError if it\n",
      "  encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
      "\n",
      "Returns:\n",
      "    A `History` object. Its `History.history` attribute is\n",
      "    a record of training loss values and metrics values\n",
      "    at successive epochs, as well as validation loss values\n",
      "    and validation metrics values (if applicable).\n",
      "\n",
      "Raises:\n",
      "    RuntimeError: 1. If the model was never compiled or,\n",
      "    2. If `model.fit` is  wrapped in `tf.function`.\n",
      "\n",
      "    ValueError: In case of mismatch between the provided input data\n",
      "        and what the model expects or when the input data is empty.\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\user\\anaconda3\\envs\\dl-dev-cpu\\lib\\site-packages\\keras\\engine\\training.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "Sequential.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-dev-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
